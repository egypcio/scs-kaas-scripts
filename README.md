# scs-training-kaas-scripts
Some helpful snippets of code to automate the creation of Cluster-Stacks

## Goals
Creating cluster stacks and clusters based on these takes a significant
number of steps that are hard to remember correctly for people that are
not cluster-API (capi) and Cluster Class experts.

We thus hide them in numbered scripts that perform these steps.
The reason for not doing everything in one script is that you do register
cloud secrets or install capi much less often than install cluster classses
which happens much less often than creating clusters.

Please refer to the [Cluster Stacks SCS training docs](https://github.com/SovereignCloudStack/scs-training/tree/main/clusterstacks)
and the [SCS docs CS quick start guide](https://docs.scs.community/docs/container/components/cluster-stacks/components/cluster-stacks/providers/openstack/quickstart)
if you prefer to do things manually. The scripts here are modeled after
the manual steps (except that we don't use the helm helper chart any
more for cluster secret creation and have separated the secrets creation
for capo/orc (04) from the per-cluster one (07)) and are referenced
in the training docs.

## Settings
There is a [cluster-settings-template.env](cluster-settings-template.env) file
that contains the parameters typically adjusted by users. Please create a
copy, fill it in, and pass it to the scripts. The settings are documented
by the comments, please study them.

## Scripts

Note: All scripts should be idempotent. Running them again will not cause
any changes if you did not change any settings.

### Once per management host
* `00-bootstrap-vm-cs.sh`: Install the needed software to be able to do
  cluster management on this host. (Developed for Debian 12.)
  This is only needed if you do not have the needed tools (jq, yq, git,
  openstackclient, docker, kind, kubectl, clusterctl, helm) preinstalled.
* `01-kind-cluster.sh`: Create kind cluster
* `02-deploy-capi.sh`: Install ORC and CAPI.
* `03-deploy-cso.sh`: Install the Cluster Stack Operator.

> Should you be installing that manually (to get a closer and better understanding of the entire process), it may be possible that you would be presented errors related to the user not still be part of the `docker` group. To solve that you can run the scripts using `newgrp`.

    * Here's how you could run scripts with `newgrp`

    ```sh
    newgrp -c "bash 01-kind-cluster.sh" docker
    ```

### Once per OpenStack Project in which we want to install clusters (per namespace)
* `04-cloud-secret.sh`: Create namespace and secrets for CAPO and ORC to 
  work with the wanted OpenStack project.

### Once per Kubernetes aka CS version (maj.min)
* `05-deploy-cstack.sh`: Create Cluster Stacks which are templates
  for various clusters with the same major minor version of k8s.
  Triggers cluster class creation and image registration.
* `06-wait-clusterclass.sh`: Wait for the cluster class needed by your
  workload cluster.

### Once per cluster
* `07-cluster-secret.sh`: Create cluster secrets.
  Optionally (by setting `CL_APPCRED_LIFETIME`) creating and rotating application
  credentials (AppCreds) that are individual per cluster. You need openstackclient tools
  installed to make this work. Call this script regularly for AppCred rotation.
* `08-create-cluster.sh`: Create a workload cluster as per all the settings
  that are passed.
* `09-wait-cluster.sh`: Wait for the workload cluster and extract KUBECONFIG.

### CSI Cinder fixup
The `cloud.conf` generated by the helm openstack-csp-helper in step 04
references a `ca-file=/etc/config/cacert` if a custom CA file is being
detected. This works for the OpenStack Cloud Controller Manager (OCCM)
that is deployed into the workload cluster, but not for the CSI Cinder
driver which mounts the `cloud-config` secrets at `/etc/kubernetes`
and not `/etc/config/` by default, so the `ca-file=` reference points
to a non-existing file, see [ClusterStacks issue #188](https://github.com/SovereignCloudStack/cluster-stacks/issues/188).
This results in a `CrashLoopBackup` state for
the `openstack-cinder-csi-*` pods in the workload cluster. Until this
is fixed up upstream, it can be patched using the `_10-old-fixup-cinder.sh`.
The fix has been accepted upstream and will be in the cinder-csi-2.33 driver.
Note that the workaround is also not required any longer for the new
`clouds.yaml` configuration style of scs2 series cluster stacks.

### OVN loadbalancer fixup
The new way on configuring OCCM's credentials for the cloud consist
of setting `use-clouds = true` in the `[Global]` swection of `cloud.conf`.
The `cloud.conf` is auto-generated, and I have not yet found a way to
tweak the process. The script `_11-fixup-ovn-lb.sh` does change the
`ccm-cloud-config` secret to enable the ovn LB provider and restarts
the OCCM.

### Testing and utilities
* `20-install-gateway-api-crd.sh`: Install gateway API CRDs.
  You will need to reinstall Cilium with the right settings to get
  Gateway API functionality from it though.
* `21-deploy-kube-dashboard.sh`: Deploys the kubernetes dashboard into
  your workload cluster. It is exposed via a Loadbalancer.
  You need to use the generated token to authenticate.

### Cleanup
* `50-remove-kube-dashboard.sh`: Removes the kubernetes dashboard if deployed
  previously (11)
* `56-cleanup-cluster.sh`: Remove loadbalancers and persistent volumes from cluster.
* `57-delete-cluster.sh`: Remove cluster again.
* `59-delete-kind.sh`: Remove the kind cluster that we used as management cluster.


### Moving management to a new cluster
The bootstrap KinD management cluster (host) is not set up in a way
that is robust enough for long-term operation. So we should move the
CAPI, CAPO, CSO management objects into a more resilient k8s cluster,
as is supported with `clusterctl move`. Beyond the capi, capi resources,
this requires tracking all other management resources (objects from
ORC, CSO etc.) -- once this is complete, a script will be offered
to facilitate this.
